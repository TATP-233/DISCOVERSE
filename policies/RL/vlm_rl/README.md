# VLM-RL: Vision-Language Model Guided Reinforcement Learning

This module enables reinforcement learning with rewards automatically generated by Vision-Language Models (VLM). Instead of manually designing reward functions, the system:

1. **Proposes candidate keypoints** from segmentation masks + depth
2. **Renders annotated images** with numbered keypoints
3. **Uses VLM (GPT-4o)** to generate constraint functions based on task instructions
4. **Converts constraints to rewards** for PPO training

## Architecture

```
┌────────────────────────────────────────────────────────────────────────────┐
│                      VLM-RL Training System                                │
├────────────────────────────────────────────────────────────────────────────┤
│                                                                            │
│   Phase 1: Constraint Generation (run once)                                │
│   ┌───────────────┐     ┌──────────────────┐     ┌───────────────────┐     │
│   │ MuJoCo Scene  │ ──▶ │  Keypoint        │ ──▶ │ VLM (GPT-4o)      │     │
│   │ + Instruction │     │  Proposer        │     │ Constraint Gen    │     │
│   └───────────────┘     └──────────────────┘     └───────────────────┘     │
│                                │                           │               │
│                                ▼                           ▼               │
│                         ┌──────────────┐           ┌──────────────┐        │
│                         │ Annotated    │           │ Constraint   │        │
│                         │ Image        │           │ Functions    │        │
│                         └──────────────┘           └──────────────┘        │
│                                                                            │
│   Phase 2: RL Training                                                     │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │                         Training Loop                             │    │
│   │  ┌─────────────┐    ┌─────────────────┐    ┌─────────────────┐    │    │
│   │  │ PPO Agent   │◀──▶│  VLMRLEnv       │◀──▶│  MuJoCo         │    │    │
│   │  │ (SBX)       │    │  (Gymnasium)    │    │  Simulation     │    │    │
│   │  └─────────────┘    └─────────────────┘    └─────────────────┘    │    │
│   │                              │                                    │    │
│   │                              ▼                                    │    │
│   │                     ┌─────────────────┐                           │    │
│   │                     │  Reward Adapter │                           │    │
│   │                     │  (Constraint→R) │                           │    │
│   │                     └─────────────────┘                           │    │
│   └───────────────────────────────────────────────────────────────────┘    │
│                                                                            │
└────────────────────────────────────────────────────────────────────────────┘
```

## Installation

```bash
# Ensure DISCOVERSE is installed
cd /path/to/DISCOVERSE
pip install -e .

# Install additional dependencies
pip install sbx-rl opencv-python openai pyyaml
```

## Quick Start

### Step 1: Generate Constraints

```bash
cd policies/RL/vlm_rl

# Set OpenAI API key
export OPENAI_API_KEY="your-api-key"

# Generate constraints for pick_kiwi task
python scripts/generate_constraints.py \
    --config configs/tasks/pick_kiwi.yaml \
    --instruction "Pick up the kiwi and place it on the white plate"
```

This will:
- Load the MuJoCo scene
- Sample keypoints from object masks (center + FPS)
- Render an annotated image
- Call GPT-4o to generate constraint functions
- Save results to `outputs/pick_kiwi/constraints/`

### Step 2: Train Policy

```bash
python scripts/train.py \
    --config configs/tasks/pick_kiwi.yaml \
    --constraints_dir outputs/pick_kiwi/constraints \
    --total_timesteps 1000000
```

### Step 3: Evaluate

```bash
python scripts/inference.py \
    --config configs/tasks/pick_kiwi.yaml \
    --model_path outputs/pick_kiwi/train_*/models/final_model \
    --constraints_dir outputs/pick_kiwi/constraints \
    --episodes 100 \
    --render
```

## Project Structure

```
vlm_rl/
├── src/
│   ├── __init__.py
│   ├── keypoint_proposer.py    # Sample keypoints from segmentation + depth
│   ├── annotated_renderer.py   # Render scene with keypoint annotations
│   ├── constraint_generator.py # VLM constraint generation
│   ├── keypoint_tracker.py     # Track keypoints during simulation
│   ├── reward_adapter.py       # Convert constraints to rewards
│   └── env.py                  # Gymnasium environment
│
├── configs/
│   ├── default.yaml            # Default configuration
│   └── tasks/
│       └── pick_kiwi.yaml      # Task-specific config
│
├── prompts/
│   └── constraint_prompt.txt   # VLM prompt template
│
├── scripts/
│   ├── convert_dataset.py      # Convert dataset to MuJoCo format
│   ├── generate_constraints.py # Step 1: Generate constraints
│   ├── train.py                # Step 2: Train policy
│   └── inference.py            # Step 3: Evaluate
│
├── assets/                     # Scene assets (meshes, MJCF files)
│   ├── meshes/                 # Object and robot mesh files
│   └── mjcf/                   # MuJoCo scene files
│
├── outputs/                    # Generated outputs
│   └── <task_name>/
│       ├── constraints/        # VLM-generated constraints
│       └── train_*/            # Training logs and models
│
└── README.md
```

## Dataset Conversion

Convert 3D scanned datasets to MuJoCo-compatible format:

### Expected Dataset Structure

```
dataset_dir/
├── 3d_assets/
│   ├── {object_name}.obj      # Object mesh files
│   ├── {object_name}.ply      # Gaussian splatting (optional)
│   └── bg_*.ply               # Background files (ignored)
├── masks/                      # Segmentation masks (optional)
├── input_image.jpg            # Scene image (optional)
└── *.npy                       # Camera parameters (optional)
```

### Convert Dataset

```bash
# Basic conversion (outputs to assets/<dataset_name>/)
python scripts/convert_dataset.py /path/to/dataset

# Custom output directory and options
python scripts/convert_dataset.py /path/to/dataset \
    --output ./my_scene \
    --name custom_scene_name \
    --table-width 1.0 \
    --table-depth 0.7 \
    --table-height 0.8 \
    --test
```

Options:
- `--output, -o`: Output directory (default: `assets/<dataset_name>`)
- `--name, -n`: Scene name (default: dataset directory name)
- `--no-robot`: Create scene without robot
- `--table-width`: Table width in meters (default: 1.0)
- `--table-depth`: Table depth in meters (default: 0.7)
- `--table-height`: Table height in meters (default: 0.75)
- `--test`: Test loading the generated scene

Output structure:
```
my_scene/
├── meshes/
│   ├── bottle.obj
│   ├── bottle_part_0.obj
│   ├── franka/
│   ├── robotiq/
│   └── ...
├── mjcf/
│   └── custom_scene_name.xml
└── metadata.json
```

The script will:
1. Scan for OBJ files in `3d_assets/`
2. Run convex decomposition (CoACD) to generate collision meshes (required)
3. Copy meshes/convex parts and generate MJCF scene file
4. Include Franka Panda + Robotiq gripper (unless `--no-robot`)

---

## Development Log

### 2026-01-26: Put Bottle Into Pot Task (Keypoint Pipeline Update)

- Added **2D keypoint pipeline** using MuJoCo segmentation + depth:
  - Sample keypoints from object masks (center point + FPS).
  - Back-project 2D keypoints to 3D for constraint/reward usage.
- Annotated renderer now supports **segmentation rendering** and **2D keypoint overlay**.
- New config flags: `keypoint_include_center`, `keypoint_depth_search_radius`.
- Removed legacy 3D surface-sampling proposer (2D pipeline is now default).
- Standalone and DISCOVERSE constraint generation scripts updated to support 2D mode.


Notes:
- Convex decomposition requires `coacd` and `trimesh` (`pip install coacd trimesh`). Failures raise an error.
- Robot assets are expected under `assets/meshes/franka`, `assets/meshes/robotiq`, and `assets/mjcf/panda_robotiq.xml` (synced from roboArena).
- `convert_dataset.py` calls `mesh2mjcf.py` internally for convex decomposition, so you typically don't need to run `mesh2mjcf.py` directly.

Example: convert new-desk dataset
```bash
python scripts/convert_dataset.py /home/zoyo/Projects/DISCOVERSE/data/new-desk \
  --output /home/zoyo/Projects/DISCOVERSE/policies/RL/vlm_rl/assets/new_desk \
  --test
```

### View Generated Scene

```bash
python -m mujoco.viewer --mjcf=assets/<dataset_name>/mjcf/<scene_name>.xml
```

## Key Concepts

### Keypoint Proposal

Instead of predefined semantic keypoints (like "handle", "spout"), we:
1. Render segmentation masks and sample 2D points (center + FPS)
2. Back-project 2D points to 3D using depth + camera parameters
3. Let the VLM decide which points are semantically meaningful

This approach (inspired by [ReKep](https://rekep-robot.github.io/)) requires zero manual annotation.

### Constraint Functions

VLM generates Python constraint functions with this signature:

```python
def stage{N}_{type}_constraint{M}(end_effector, keypoints):
    """
    Args:
        end_effector: np.ndarray [3] - end effector position
        keypoints: np.ndarray [K, 3] - all keypoint positions

    Returns:
        cost: float - negative/zero means satisfied, positive means violated
    """
    return cost
```

Types:
- **subgoal**: Must be satisfied at stage end
- **path**: Must be satisfied throughout the stage

### Multi-Stage Tasks

Tasks are decomposed into stages (e.g., approach → grasp → transport → place).
Each stage has its own constraints, and the reward adapter tracks progress.

## Configuration

### Task Configuration (`configs/tasks/*.yaml`)

```yaml
task_name: "my_task"
task_module: "discoverse.examples.tasks_mmk2.my_task"

instruction: "Natural language task description"

object_bodies:
  - "object1"
  - "object2"

end_effector_body: "gripper_link"
points_per_object: 5
keypoint_include_center: true
keypoint_depth_search_radius: 6

reward_config:
  reward_type: "negative"  # negative, exponential, sparse, tanh
  subgoal_weight: 2.0
  path_weight: 1.0
```

### Adding a New Task

1. Create MuJoCo model (`.xml`) and task module
2. Create config file in `configs/tasks/`
3. Run `generate_constraints.py` with your instruction
4. Train with `train.py`

## Comparison with Manual Reward Design

| Aspect | Manual | VLM-RL |
|--------|--------|--------|
| Setup effort | High (design reward) | Low (write instruction) |
| New tasks | Requires engineering | Just change instruction |
| Interpretability | Depends on design | Constraints are readable |
| Performance | Optimized | Depends on VLM quality |

## Troubleshooting

### VLM Generation Issues

- Ensure `OPENAI_API_KEY` is set
- Check `outputs/*/constraints/raw_output.txt` for VLM response
- Adjust prompt in `prompts/constraint_prompt.txt` if needed

### Training Issues

- Check constraint files for syntax errors
- Enable `--render` to visualize behavior
- Adjust reward weights in config

## References

- [ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation](https://rekep-robot.github.io/)
- [DISCOVERSE Simulation Framework](https://github.com/DISCOVER-Robotics/DISCOVERSE)
- [Stable Baselines3 (SBX)](https://github.com/araffin/sbx)
