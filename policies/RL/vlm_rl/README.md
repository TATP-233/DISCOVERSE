# VLM-RL: Vision-Language Model Guided Reinforcement Learning

This module enables reinforcement learning with rewards automatically generated by Vision-Language Models (VLM). Instead of manually designing reward functions, the system:

1. **Proposes candidate keypoints** from MuJoCo scene geometry
2. **Renders annotated images** with numbered keypoints
3. **Uses VLM (GPT-4o)** to generate constraint functions based on task instructions
4. **Converts constraints to rewards** for PPO training

## Architecture

```
┌────────────────────────────────────────────────────────────────────────────┐
│                      VLM-RL Training System                                │
├────────────────────────────────────────────────────────────────────────────┤
│                                                                            │
│   Phase 1: Constraint Generation (run once)                                │
│   ┌───────────────┐     ┌──────────────────┐     ┌───────────────────┐     │
│   │ MuJoCo Scene  │ ──▶ │  Keypoint        │ ──▶ │ VLM (GPT-4o)      │     │
│   │ + Instruction │     │  Proposer        │     │ Constraint Gen    │     │
│   └───────────────┘     └──────────────────┘     └───────────────────┘     │
│                                │                           │               │
│                                ▼                           ▼               │
│                         ┌──────────────┐           ┌──────────────┐        │
│                         │ Annotated    │           │ Constraint   │        │
│                         │ Image        │           │ Functions    │        │
│                         └──────────────┘           └──────────────┘        │
│                                                                            │
│   Phase 2: RL Training                                                     │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │                         Training Loop                             │    │
│   │  ┌─────────────┐    ┌─────────────────┐    ┌─────────────────┐    │    │
│   │  │ PPO Agent   │◀──▶│  VLMRLEnv       │◀──▶│  MuJoCo         │    │    │
│   │  │ (SBX)       │    │  (Gymnasium)    │    │  Simulation     │    │    │
│   │  └─────────────┘    └─────────────────┘    └─────────────────┘    │    │
│   │                              │                                    │    │
│   │                              ▼                                    │    │
│   │                     ┌─────────────────┐                           │    │
│   │                     │  Reward Adapter │                           │    │
│   │                     │  (Constraint→R) │                           │    │
│   │                     └─────────────────┘                           │    │
│   └───────────────────────────────────────────────────────────────────┘    │
│                                                                            │
└────────────────────────────────────────────────────────────────────────────┘
```

## Installation

```bash
# Ensure DISCOVERSE is installed
cd /path/to/DISCOVERSE
pip install -e .

# Install additional dependencies
pip install sbx-rl opencv-python openai pyyaml
```

## Quick Start

### Step 1: Generate Constraints

```bash
cd policies/RL/vlm_rl

# Set OpenAI API key
export OPENAI_API_KEY="your-api-key"

# Generate constraints for pick_kiwi task
python scripts/generate_constraints.py \
    --config configs/tasks/pick_kiwi.yaml \
    --instruction "Pick up the kiwi and place it on the white plate"
```

This will:
- Load the MuJoCo scene
- Generate candidate keypoints from object surfaces
- Render an annotated image
- Call GPT-4o to generate constraint functions
- Save results to `outputs/pick_kiwi/constraints/`

### Step 2: Train Policy

```bash
python scripts/train.py \
    --config configs/tasks/pick_kiwi.yaml \
    --constraints_dir outputs/pick_kiwi/constraints \
    --total_timesteps 1000000
```

### Step 3: Evaluate

```bash
python scripts/inference.py \
    --config configs/tasks/pick_kiwi.yaml \
    --model_path outputs/pick_kiwi/train_*/models/final_model \
    --constraints_dir outputs/pick_kiwi/constraints \
    --episodes 100 \
    --render
```

## Project Structure

```
vlm_rl/
├── src/
│   ├── __init__.py
│   ├── keypoint_proposer.py    # Generate keypoints from MuJoCo geometry
│   ├── annotated_renderer.py   # Render scene with keypoint annotations
│   ├── constraint_generator.py # VLM constraint generation
│   ├── keypoint_tracker.py     # Track keypoints during simulation
│   ├── reward_adapter.py       # Convert constraints to rewards
│   └── env.py                  # Gymnasium environment
│
├── configs/
│   ├── default.yaml            # Default configuration
│   └── tasks/
│       └── pick_kiwi.yaml      # Task-specific config
│
├── prompts/
│   └── constraint_prompt.txt   # VLM prompt template
│
├── scripts/
│   ├── generate_constraints.py # Step 1: Generate constraints
│   ├── train.py                # Step 2: Train policy
│   └── inference.py            # Step 3: Evaluate
│
├── outputs/                    # Generated outputs
│   └── <task_name>/
│       ├── constraints/        # VLM-generated constraints
│       └── train_*/            # Training logs and models
│
└── README.md
```

## Key Concepts

### Keypoint Proposal

Instead of predefined semantic keypoints (like "handle", "spout"), we:
1. Sample candidate points from MuJoCo geometry surfaces
2. Apply Farthest Point Sampling (FPS) for diversity
3. Let the VLM decide which points are semantically meaningful

This approach (inspired by [ReKep](https://rekep-robot.github.io/)) requires zero manual annotation.

### Constraint Functions

VLM generates Python constraint functions with this signature:

```python
def stage{N}_{type}_constraint{M}(end_effector, keypoints):
    """
    Args:
        end_effector: np.ndarray [3] - end effector position
        keypoints: np.ndarray [K, 3] - all keypoint positions

    Returns:
        cost: float - negative/zero means satisfied, positive means violated
    """
    return cost
```

Types:
- **subgoal**: Must be satisfied at stage end
- **path**: Must be satisfied throughout the stage

### Multi-Stage Tasks

Tasks are decomposed into stages (e.g., approach → grasp → transport → place).
Each stage has its own constraints, and the reward adapter tracks progress.

## Configuration

### Task Configuration (`configs/tasks/*.yaml`)

```yaml
task_name: "my_task"
task_module: "discoverse.examples.tasks_mmk2.my_task"

instruction: "Natural language task description"

object_bodies:
  - "object1"
  - "object2"

end_effector_body: "gripper_link"
points_per_object: 5

reward_config:
  reward_type: "negative"  # negative, exponential, sparse, tanh
  subgoal_weight: 2.0
  path_weight: 1.0
```

### Adding a New Task

1. Create MuJoCo model (`.xml`) and task module
2. Create config file in `configs/tasks/`
3. Run `generate_constraints.py` with your instruction
4. Train with `train.py`

## Comparison with Manual Reward Design

| Aspect | Manual | VLM-RL |
|--------|--------|--------|
| Setup effort | High (design reward) | Low (write instruction) |
| New tasks | Requires engineering | Just change instruction |
| Interpretability | Depends on design | Constraints are readable |
| Performance | Optimized | Depends on VLM quality |

## Troubleshooting

### VLM Generation Issues

- Ensure `OPENAI_API_KEY` is set
- Check `outputs/*/constraints/raw_output.txt` for VLM response
- Adjust prompt in `prompts/constraint_prompt.txt` if needed

### Training Issues

- Check constraint files for syntax errors
- Enable `--render` to visualize behavior
- Adjust reward weights in config

## References

- [ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation](https://rekep-robot.github.io/)
- [DISCOVERSE Simulation Framework](https://github.com/DISCOVER-Robotics/DISCOVERSE)
- [Stable Baselines3 (SBX)](https://github.com/araffin/sbx)
